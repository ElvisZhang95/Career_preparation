{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic data analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### About load file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "INPUT_DIR = os.getcwd()\n",
    "file_name = \"data.csv\"\n",
    "\n",
    "load_csv = INPUT_DIR + file_name\n",
    "df = pd.read_csv(load_csv, index_col=0, parse_dates=True, dayfirst=True).sort_index(ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### General analysis / Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print head of data, so show example\n",
    "df.head()\n",
    "#print feature data type\n",
    "df.dtypes\n",
    "#print data sample number\n",
    "print(\"We have {:,} market samples in the training dataset.\".format(df.shape[0]))\n",
    "#print price value\n",
    "print(\"Average Standard deviation of price change within a day in {df['price'].mean():.4f}.\")\n",
    "#check nan value \n",
    "df.isna().sum()\n",
    "#check unique number, e.g. 'time' is one of feature\n",
    "df['time'].nunique()\n",
    "#check feature name equal to unknown\n",
    "df[df['time'] == 'unknown'].size\n",
    "#get detail information about feature\n",
    "df['time'].describe()\n",
    "#filter data through specific value condition\n",
    "outliers = df[(df['time']>1)|(df['time']<-1)]\n",
    "#groupby. Let 'time' feature be the key, other feature in the table become value\n",
    "df.groupby('time') \n",
    "#limit time period\n",
    "new_df = df.loc[df['time']>='2010-01-01 22:00:00+0000']\n",
    "#value_counts, pick top 10.\n",
    "df['feature'].value_counts().head(10)\n",
    "#plot a horizontal bar plot 水平柱状图\n",
    "ax = df.plot.barh(x='lab', y='val')\n",
    "df['time'].plot('barh')\n",
    "#plot bar 柱状图\n",
    "df['time'].plot('bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Program Running time cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "start = timeit.default_timer()\n",
    "\n",
    "#Your statements here\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "\n",
    "print('Time: ', stop - start) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pandas application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature analysis (category part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method 1\n",
    "#onehot encode with scikit-learn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "data = ['warm', 'cold', 'hot']\n",
    "value = array(data)\n",
    "#integer encode\n",
    "label_encoder = LabelEncoder()\n",
    "#from 0,1,2,3,...\n",
    "integer_encoded = label_encoder.fit_transform(values)\n",
    "\n",
    "#binary encode\n",
    "onehot_encoder = OneHotEncoder(sparse= False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "\n",
    "#invert oneHot to label\n",
    "#here, [0,:] means the first example\n",
    "inverted = label_encoder.inverse_transform([argmax(onehot_encoded[0,:])])\n",
    "\n",
    "#Method 2\n",
    "#onehot encode with pandas\n",
    "onehot = pd.get_dummies(pd.Series(list(train_data['Stance'].values)))\n",
    "onehot = np.asarray(onehot_stances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature analysis (text part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import re\n",
    "stopWords = set(stopwords.words('english'))\n",
    "\n",
    "def tokenization(text):\n",
    "    token = []\n",
    "    for w in word_tokenize(text):\n",
    "        w = re.sub('[A-Za-z]+', '', w).lower()\n",
    "        if (w not in stopWords) and (len(w)>= 3) :\n",
    "            token.append(w)\n",
    "    return token\n",
    "\n",
    "#method 2, involve in regular expression\n",
    "import nltk\n",
    "mytweet = \"@john lol that was #awesome :)\"\n",
    "nltk.word_tokenize(mytweet)\n",
    "#the result is ['@','john','lol','that','was','#','awesome',':',')']\n",
    "#also we can use regular expression with function regexp_tokenize\n",
    "\n",
    "pattern = r'''(?x)                                 #verbose regex flag\n",
    "         ([A-Z]\\.)+                                #abbreviations\n",
    "        |\\d+:\\d                                    #times, e.g. 5:23\n",
    "        |(https?://)?(\\w+\\.)(\\w{2,})+([\\w/]+)      #URLs\n",
    "        |[@\\#]?\\w+(?:[-']\\w+)*                     #word,@user, words\n",
    "        |\\$\\d+(\\.\\d+)?%?                           #currency, pcts\n",
    "        |\\.\\.\\.                                    #ellipses\n",
    "        |[!?]+                                     #!!!, ???\n",
    "        '''\n",
    "nltk.regexp_tokenize(mytweet, pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF term frequency and inverse document frequency\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = ['This is the first document.', \n",
    "         'This document is the second decoument',\n",
    "         'And this is the third one.']\n",
    "vectorizer = TfidfVectorizer()\n",
    "#also, set more conditions\n",
    "#vectorizer = TfidfVectorizer(norm='l2', min_df=0, use_idf=True, smooth_idf=False,\\\n",
    "#sublinear_tf=True, tokenizer=tokenization, stop_words='english')\n",
    "\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(vectorizer.get_feature_names())\n",
    "print(X.shape)\n",
    "X = X.toarray()\n",
    "label = inverse_transform(X)\n",
    "\n",
    "#the X is the vector we transform from the text\n",
    "#further, we can use vector to compare the cosine similarity of two documents.\n",
    "\n",
    "from scipy import spatial\n",
    "def cosine_similarity(v1, v2): #vector 1 and vector 2\n",
    "    result = 1 - spatial.distance.cosine(v1, v2)\n",
    "    return result\n",
    "\n",
    "#compute similarity one by one\n",
    "cos_simility = []\n",
    "for count_0, doc_0 in enumerate(X.toarray()):\n",
    "    for count_1, doc_1 in enumberate(X.toarray()):\n",
    "        cos_simility.append(cosine_similarity(doc_0, doc_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LDA language model\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "#clean process\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "def clean(doc):\n",
    "    stop_free = \"\".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.splic())\n",
    "    return normalized\n",
    "doc_clean = [clean(doc).split() for doc in train['article body']]\n",
    "\n",
    "# !pip install -U gensim\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "#creating the term dictionary of our courpus, where every unique term is assigned an index\n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "\n",
    "#converting list of documents(corpus) into document term matrix unsing dictionary prepared above\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "\n",
    "#creating the object for LDA model using gensim library\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "#running and training LDA model on the document term matrix\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=200， id2word = dictionary, passes=1, alpha=1e-2, eta=0.5e-2, \\\n",
    "              minimum_probability=0.0)\n",
    "doc_distribution = np.array([tup[1] for tup in ldamodel.get_document_topics(bow=doc_term_matrix)])\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------\n",
    "#add new document in the lda model for testing.\n",
    "new_doc_clean = [clean(doc).split() for doc in test_stance['sss']]\n",
    "new_bow = [dictionary.doc2bow(doc) for doc in new_doc_clean]\n",
    "new_doc_distribution = np.array([[tup[1] for tup in lst] for lst in ldamodel.get_document_topics(bow=new_bow)])\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------\n",
    "#Then we can put the distribution of each document out, and then compare the KL divergence\n",
    "def KL_divergence(a,b):\n",
    "    epsilon = 1e-10\n",
    "    a = np.asarray(a, dtype= np.float)\n",
    "    b = np.asarray(b, dtype= np.float)\n",
    "    a = a+ epsilon\n",
    "    b = b+ epsilon\n",
    "    return np.sum(np.where(a!=0), a*np.log(a/b),0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature analysis (Time series part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#log return\n",
    "def log_returns(x, lag=1):\n",
    "    \"\"\"Calculate log returns between adjacent close prices\"\"\"\n",
    "    return np.log(x) - np.log(x.shift(lag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#date process\n",
    "def to_date(date_column):\n",
    "    return data_column.apply(lambda x: pd.to_datetime(x).strftime('%d-%m-%Y'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               lTid cDealable CurrencyPair  RateBid  RateAsk\n",
      "RateDateTime                                                                \n",
      "2018-09-02 17:00:06.910  6888383604         D      GBP/USD  1.29110  1.29180\n",
      "2018-09-02 17:01:12.410  6888383703         D      GBP/USD  1.29115  1.29195\n",
      "2018-09-02 17:05:26.410  6888383870         D      GBP/USD  1.29110  1.29180\n",
      "2018-09-02 17:05:56.160  6888383897         D      GBP/USD  1.29117  1.29197\n",
      "2018-09-02 17:05:56.410  6888383903         D      GBP/USD  1.29121  1.29200\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RateDateTime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-09-02 17:00:00</th>\n",
       "      <td>1.29110</td>\n",
       "      <td>1.29115</td>\n",
       "      <td>1.2911</td>\n",
       "      <td>1.29115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-02 17:05:00</th>\n",
       "      <td>1.29110</td>\n",
       "      <td>1.29200</td>\n",
       "      <td>1.2911</td>\n",
       "      <td>1.29180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-02 17:10:00</th>\n",
       "      <td>1.29165</td>\n",
       "      <td>1.29178</td>\n",
       "      <td>1.2916</td>\n",
       "      <td>1.29160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-02 17:15:00</th>\n",
       "      <td>1.29164</td>\n",
       "      <td>1.29210</td>\n",
       "      <td>1.2916</td>\n",
       "      <td>1.29160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-02 17:20:00</th>\n",
       "      <td>1.29160</td>\n",
       "      <td>1.29180</td>\n",
       "      <td>1.2916</td>\n",
       "      <td>1.29180</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        open     high     low    close\n",
       "RateDateTime                                          \n",
       "2018-09-02 17:00:00  1.29110  1.29115  1.2911  1.29115\n",
       "2018-09-02 17:05:00  1.29110  1.29200  1.2911  1.29180\n",
       "2018-09-02 17:10:00  1.29165  1.29178  1.2916  1.29160\n",
       "2018-09-02 17:15:00  1.29164  1.29210  1.2916  1.29160\n",
       "2018-09-02 17:20:00  1.29160  1.29180  1.2916  1.29180"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#time frequency\n",
    "#resample and interpolate\n",
    "#two type resample method: Upsampling, Downsampling.\n",
    "\n",
    "#method 1:\n",
    "df_1 = pd.read_csv('GBP_USD_Week1.csv', \\\n",
    "                         index_col=3, parse_dates=True)\n",
    "print(df_1.head())\n",
    "# # names=['Symbol', 'Date_Time', 'Bid', 'Ask']\n",
    "#use L for milliseconds, U for microseconds, and S for seconds.\n",
    "data_ask =  df_1['RateBid'].resample('5Min').ohlc()\n",
    "data_bid =  df_1['RateAsk'].resample('5Min').ohlc()\n",
    "#combine\n",
    "data_ask_bid=pd.concat([data_ask, data_bid], axis=1, keys=['Ask', 'Bid'])\n",
    "data_ask.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "#method 2:\n",
    "df_2 = pd.read_csv('GBP_USD_Week1.csv', \\\n",
    "                         index_col=0, parse_dates=True)\n",
    "\n",
    "from datetime import datetime\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "def datetime_to_timestamp(datetime_obj):\n",
    "    \"\"\"将本地(local) datetime 格式的时间 (含毫秒) 转为毫秒时间戳\n",
    "    :param datetime_obj: {datetime}2016-02-25 20:21:04.242000\n",
    "    :return: 13 位的毫秒时间戳  1456402864242\n",
    "    \"\"\"\n",
    "    local_timestamp = np.long(time.mktime(datetime_obj.timetuple()) * 1000.0 + datetime_obj.microsecond / 1000.0)\n",
    "    return local_timestamp\n",
    "\n",
    "def strtime_to_datetime(timestr):\n",
    "    \"\"\"将字符串格式的时间 (含毫秒) 转为 datetiem 格式\n",
    "    :param timestr: {str}'2016-02-25 20:21:04.242'\n",
    "    :return: {datetime}2016-02-25 20:21:04.242000\n",
    "    \"\"\"\n",
    "    timestr = timestr[:-3]\n",
    "    local_datetime = datetime.strptime(timestr, \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "    return local_datetime\n",
    "\n",
    "def strtime_to_timestamp(local_timestr):\n",
    "    \"\"\"将本地时间 (字符串格式，含毫秒) 转为 13 位整数的毫秒时间戳\n",
    "    :param local_timestr: {str}'2016-02-25 20:21:04.242'\n",
    "    :return: 1456402864242\n",
    "    \"\"\"\n",
    "    local_datetime = strtime_to_datetime(local_timestr)\n",
    "    timestamp = datetime_to_timestamp(local_datetime)\n",
    "    return timestamp\n",
    "\n",
    "# time cost: 6.639142935929584\n",
    "def str_to_ts(timestr):\n",
    "    fmt = \"%Y-%m-%d %H:%M:%S.%f\"\n",
    "    try:\n",
    "        local_datetime = datetime.strptime(timestr, fmt)\n",
    "    except ValueError as v:\n",
    "        if 'unconverted data remains: ' in str(v):\n",
    "            ulr = len(v.args[0].partition('unconverted data remains: ')[2])\n",
    "            if ulr:\n",
    "                local_datetime = datetime.strptime(timestr[:-ulr], fmt)\n",
    "                \n",
    "        elif \"time data %r does not match format %r\" %(timestr, fmt) in str(v):\n",
    "            \n",
    "            diff = 26-len(timestr)-1\n",
    "            seq = np.zeros(diff,int)\n",
    "            seq = seq.astype(str)\n",
    "            if len(timestr)>19:\n",
    "                timestr = timestr+ \"\".join(seq)\n",
    "            else:\n",
    "                timestr = timestr+'.'+\"\".join(seq)\n",
    "            local_datetime = datetime.strptime(timestr, fmt)\n",
    "        else:\n",
    "            raise v\n",
    "                \n",
    "    timestamp = datetime_to_timestamp(local_datetime) \n",
    "    return timestamp\n",
    "\n",
    "#time cost: 4.61269746204043\n",
    "def str_to_ts_(timestr):\n",
    "    \n",
    "    fmt = \"%Y-%m-%d %H:%M:%S.%f\"\n",
    "    if len(timestr) in range(19,26):\n",
    "        diff = 26-len(timestr)-1\n",
    "        seq = np.zeros(diff,int)\n",
    "        seq = seq.astype(str)\n",
    "        if len(timestr)==19:\n",
    "            timestr = timestr+'.'+\"\".join(seq)\n",
    "        else:\n",
    "            timestr = timestr+ \"\".join(seq)\n",
    "        local_datetime = datetime.strptime(timestr, fmt)\n",
    "\n",
    "    elif len(timestr)>=26:\n",
    "        ulr = len(timestr)-26\n",
    "        local_datetime = datetime.strptime(timestr[:-ulr], fmt)\n",
    "    else:\n",
    "        print('ValueError')\n",
    "                \n",
    "    timestamp = datetime_to_timestamp(local_datetime) \n",
    "    return timestamp\n",
    "\n",
    "\n",
    "l = [(lambda x: str_to_ts_(x))(x) for x in df_2['RateDateTime'].values]\n",
    "df_2['timestamp'] = l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cDealable</th>\n",
       "      <th>CurrencyPair</th>\n",
       "      <th>RateDateTime</th>\n",
       "      <th>RateBid</th>\n",
       "      <th>RateAsk</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lTid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6888383604</th>\n",
       "      <td>D</td>\n",
       "      <td>GBP/USD</td>\n",
       "      <td>2018-09-02 17:00:06.910000000</td>\n",
       "      <td>1.29110</td>\n",
       "      <td>1.29180</td>\n",
       "      <td>1535904006910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6888383703</th>\n",
       "      <td>D</td>\n",
       "      <td>GBP/USD</td>\n",
       "      <td>2018-09-02 17:01:12.410000000</td>\n",
       "      <td>1.29115</td>\n",
       "      <td>1.29195</td>\n",
       "      <td>1535904072410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6888383870</th>\n",
       "      <td>D</td>\n",
       "      <td>GBP/USD</td>\n",
       "      <td>2018-09-02 17:05:26.410000000</td>\n",
       "      <td>1.29110</td>\n",
       "      <td>1.29180</td>\n",
       "      <td>1535904326410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6888383897</th>\n",
       "      <td>D</td>\n",
       "      <td>GBP/USD</td>\n",
       "      <td>2018-09-02 17:05:56.160000000</td>\n",
       "      <td>1.29117</td>\n",
       "      <td>1.29197</td>\n",
       "      <td>1535904356160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6888383903</th>\n",
       "      <td>D</td>\n",
       "      <td>GBP/USD</td>\n",
       "      <td>2018-09-02 17:05:56.410000000</td>\n",
       "      <td>1.29121</td>\n",
       "      <td>1.29200</td>\n",
       "      <td>1535904356410</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           cDealable CurrencyPair                   RateDateTime  RateBid  \\\n",
       "lTid                                                                        \n",
       "6888383604         D      GBP/USD  2018-09-02 17:00:06.910000000  1.29110   \n",
       "6888383703         D      GBP/USD  2018-09-02 17:01:12.410000000  1.29115   \n",
       "6888383870         D      GBP/USD  2018-09-02 17:05:26.410000000  1.29110   \n",
       "6888383897         D      GBP/USD  2018-09-02 17:05:56.160000000  1.29117   \n",
       "6888383903         D      GBP/USD  2018-09-02 17:05:56.410000000  1.29121   \n",
       "\n",
       "            RateAsk      timestamp  \n",
       "lTid                                \n",
       "6888383604  1.29180  1535904006910  \n",
       "6888383703  1.29195  1535904072410  \n",
       "6888383870  1.29180  1535904326410  \n",
       "6888383897  1.29197  1535904356160  \n",
       "6888383903  1.29200  1535904356410  "
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "#or astype('f') for float32\n",
    "df_['bucket'] = df_['timestamp'].astype('f') // 10**9 //60 // 1\n",
    "\n",
    "df_bucket = df_.groupby('bucket').mean()\n",
    "df_bucket['timestamp'] = pd.to_datetime(df_bucket.index* 10**9 * 60 * 1)\n",
    "#df_\n",
    "\n",
    "# def groupByMinutes(df, num_min):\n",
    "#     df['bucket'] = df['timestamp'].astype(int) // 10**9 // 60 // num_min #num_min is the minutes we want to set\n",
    "#     df_bucket = df.groupby('bucket').mean()\n",
    "#     df_bucket['timestamp'] = pd.to_datetime(df_bucket.index * 10**9 * 60 * num_min)\n",
    "#     df_bucket['num_tweets'] = df.groupby('bucket').size() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ETL extract, clean, transform, load\n",
    "#extract, combine available feature into one table\n",
    "def extract():\n",
    "    #df = pd.DataFrame()\n",
    "    lme_stocks = data['lme_stocks']\n",
    "    frames = [lme_stocks, lme_prices, ted_, ted_spread_, bdi_, vix_, gsci_, shfe_price]\n",
    "    df = pd.concat(frames, axis = 1)\n",
    "    df.columns = ['lme_stocks', 'lme_prices', 'ted', 'ted_spread', 'bdi', 'vix', 'gsci', 'shfe_price']\n",
    "    \n",
    "def clean():\n",
    "    logging.warning('Starting to clean data')\n",
    "    #remove the label is null\n",
    "    df.dropna(axis=0, how='any', subset=['shfe_price'], inplace=True)\n",
    "    check = df.isnull().values.any()\n",
    "    \n",
    "    # method 1: fill nan with average value\n",
    "    #if check:\n",
    "    #    mean = df.mean #mean of each column of the dataframe\n",
    "    #    replace_values = {'Open': mean[0], 'High':mean[1], 'Close':mean[2]}\n",
    "    #    df = df.fillna(value= replace_values)\n",
    "    #    \n",
    "    #Converting stock values to float and Volumns to int\n",
    "    #df[['Open', 'High', 'Close']] = df[['Open', 'High', 'Close']].astyoe(float)\n",
    "    #df['Volumn'] = df['Volumn'].astype(int)\n",
    "    #\n",
    "    #\n",
    "    # method 2: fill nan with last value\n",
    "    if check:\n",
    "        #fill nan by forward value \n",
    "        df = df.replace([np.inf, -np.inf], np.nan).fillna(method='ffill')\n",
    "    \n",
    "def transform():\n",
    "    logging.warning('Performing data transformation')\n",
    "    #making new column\n",
    "    #log return or percentage change\n",
    "    df['log_return_price'] = log_return(df['log_return_price'])\n",
    "    \n",
    "    # Find first and last dates for which all features available\n",
    "    start_date = df.loc[df.notnull().all(axis=1)].index.min()\n",
    "    end_date = df.loc[df.notnull().all(axis=1)].index.max()\n",
    "    df = df.loc[start_date:end_date, :]\n",
    "    \n",
    "    # Maybe no need\n",
    "    #normalised to zero mean and standard variance\n",
    "    #for item in df_Xy.columns:\n",
    "    #    df_Xy[item] = preprocessing.scale(df_Xy[item])     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check mean, std, and plot to show result.\n",
    "#function to plot multi-figures\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(10,10))\n",
    "axes[0,0].plot(cu_log_return.index,cu_log_return[['shfe_price']])\n",
    "axes[0,0].set_title('Cu_Shfe_price')\n",
    "axes[0,1].plot(cu_log_return.index,cu_log_return[['lme_stocks']])\n",
    "axes[0,1].set_title('Cu_lme_stocks')\n",
    "axes[1,0].plot(cu_log_return.index,cu_log_return[['lme_prices']])\n",
    "axes[1,0].set_title('Cu_lme_prices')\n",
    "axes[1,1].plot(cu_log_return.index,cu_log_return[['ted']])\n",
    "axes[1,1].set_title('ted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot autocorrelation and partial autocorrelation to show stationary.\n",
    "import statsmodels.api as sm  \n",
    "def plot_acf_pacf(input_data):   \n",
    "    fig = plt.figure(figsize=(12,8))\n",
    "    ax1 = fig.add_subplot(211)\n",
    "    fig = sm.graphics.tsa.plot_acf(input_data, lags=40, alpha=.05, ax=ax1)\n",
    "    ax2 = fig.add_subplot(212)\n",
    "    fig = sm.graphics.tsa.plot_pacf(input_data, lags=40, alpha=.05, ax=ax2)\n",
    "    print(\"ACF and PACF of input_data\")\n",
    "    plt.show()\n",
    "    \n",
    "#or use Dickey-Fuller test to check stationary.\n",
    "from pandas import tseries\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "def adf_test(input_data, feature_name):\n",
    "    # perform Augmented Dickey Fuller test\n",
    "    print('Results of Augmented Dickey-Fuller test:', feature_name)\n",
    "    test_data = input_data[[feature_name]]\n",
    "    y = test_data.values[:,0]\n",
    "    dftest = adfuller(y, autolag='AIC')\n",
    "    dfoutput = pd.Series(dftest[0:4], index=['test statistic', 'p-value', '# of lags', '# of observations'])\n",
    "    for key, value in dftest[4].items():\n",
    "        dfoutput['Critical Value ({})'.format(key)] = value\n",
    "    print(dftest)\n",
    "    #print(dfoutput)\n",
    "#     if dftest[1]<0.05 and dftest[4].get('5%')<0.05:\n",
    "#         print(feature_name, 'is stationary.')\n",
    "#         print('test statistic', dftest[0])\n",
    "#         print('p-value = ', dftest[1])\n",
    "#         print(type(dftest[1]))\n",
    "#         print('Critical Value (5%) = ', dftest[4].get('5%'))\n",
    "#     else: \n",
    "#         print(feature, 'is not stationary.')\n",
    "adf_test(al_log_return, 'shfe_price')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature cross correlation\n",
    "#method 1: numpy and panda\n",
    "corr = df.corr()\n",
    "corr.style.background_gradient()\n",
    "#also\n",
    "corr.style.background_gradient.set_precision(2)\n",
    "\n",
    "#method 2: \n",
    "import matplotlib.pyplot as plt\n",
    "#matplotlib.style.use('ggplot')\n",
    "plt.imshow(X.corr(), camp= plt.cm.Reds, interpolation='nearest')\n",
    "plt.colorbar()\n",
    "tick_marks = [i for i in range(len(X.columns))]\n",
    "plt.xticks(tick_marks, X.columns, rotation='vertical')\n",
    "plt.yticks(tick_marks, X.columns)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Unbalanced data set solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set weights?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = model_selection.train_test_split(X, Y, test_size=0.1, random_state=99)\n",
    "\n",
    "xgb = XGBClassifier(n_jobs=4, n_estimators=300, max_depth=6, eta=0.15)\n",
    "xgb.fit(X_train, Y_train)\n",
    "print(\"Accuracy Score: \", accuracy_score(xgb.predict(X_test), Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gaussian Process Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_process(X, Y, x_pred):\n",
    "       \n",
    "    X = X.values\n",
    "    Y = Y.values\n",
    "    x_pred = x_pred.values\n",
    "    kernels = 1.0 * DotProduct(sigma_0=1.0, sigma_0_bounds=(1e-05, 100000.0)) \\\n",
    "#     +1.0 * RationalQuadratic(length_scale=1.0) \\\n",
    "#     + 1.0 * WhiteKernel(noise_level=1e-1) \\\n",
    "#     + 1.0 * Matern(length_scale=1.0, nu=1.5) \\\n",
    "#     + 1.0 * RBF(length_scale=1.0, length_scale_bounds=(1e-05, 100000.0))\\\n",
    "#     + 1.0 * ExpSineSquared(length_scale=1.0, periodicity=1.0, length_scale_bounds=(1e-05, 100000.0), periodicity_bounds=(1e-05, 100000.0))\n",
    "#     + 1.0 * ConstantKernel(constant_value=1.0, constant_value_bounds=(1e-05, 100000.0))\\\n",
    "        \n",
    "    gp = GaussianProcessRegressor(kernel=kernels, n_restarts_optimizer=10, alpha = 0)\n",
    "    \n",
    "    # Fit to data using Maximum Likelihood Estimation of the parameters\n",
    "    # learn the hyperparameters and scale of each kernel\n",
    "    gp.fit(X, Y)\n",
    "\n",
    "    \n",
    "    forecast= gp.predict(x_pred, return_std=True, return_cov=False)\n",
    "    y_pred = forecast[0][0]\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Time series evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from numpy.linalg import norm\n",
    "from dtw import dtw\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "def evaluate(model_name, org_price, pred_price):\n",
    "    \n",
    "    org_price = org_price.reshape(len(org_price),1)\n",
    "    pred_price = pred_price.reshape(len(pred_price),1)\n",
    "    \n",
    "    mse = compute_mse_error(org_price, pred_price)\n",
    "    mae = compute_mean_absolute_error(org_price, pred_price)\n",
    "    rmse =compute_root_mean_square_error(org_price, pred_price)\n",
    "    dtw_dist, dtw_cost, dtw_acc, dtw_path = compute_dtw(org_price, pred_price)\n",
    "    corr_rank, p_value = compute_spearmanr(org_price, pred_price)\n",
    "    \n",
    "    df = pd.DataFrame(index=[model_name],columns=['MSE', 'MAE', 'RMSE' ,'DTW', 'Spearmanr', 'p-value'])\n",
    "    df.set_value(model_name, 'MSE', mse)\n",
    "    df.set_value(model_name, 'MAE', mae)\n",
    "    df.set_value(model_name, 'RMSE', rmse)\n",
    "    df.set_value(model_name, 'DTW', dtw_cost[-1][-1])\n",
    "    df.set_value(model_name, 'Spearmanr', corr_rank)\n",
    "    df.set_value(model_name, 'p_value', p_value)\n",
    "    return df\n",
    "\n",
    "def compute_daily_error(org_price, pred_price):\n",
    "    return org_price - pred_price\n",
    "\n",
    "#MSE\n",
    "def compute_mse_error(org_price, pred_price):\n",
    "    return mean_squared_error(org_price, pred_price)\n",
    "\n",
    "#MAE\n",
    "def compute_mean_absolute_error(org_price, pred_price):  \n",
    "    return mean_absolute_error(org_price, pred_price)\n",
    "\n",
    "#RMSE\n",
    "def compute_root_mean_square_error(org_price, pred_price):\n",
    "    return np.sqrt(((org_price - pred_price) ** 2).mean())\n",
    "\n",
    "#smaller dtw Accumulated Distortion(cost[-1][-1]) means more similar time series\n",
    "def compute_dtw(x, y):   #Dynamic time warping\n",
    "    dist, cost, acc, path = dtw(x, y, dist=lambda x, y: norm(x - y, ord=1))\n",
    "    return dist, cost, acc, path\n",
    "\n",
    "def compute_spearmanr(org_price, pred_price):\n",
    "    correlation_rank = spearmanr(org_price, pred_price)\n",
    "    return correlation_rank[0], correlation_rank[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ROC draw\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "fpr=dict()\n",
    "tpr=dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(predictions_test, outcome_test)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=1, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
