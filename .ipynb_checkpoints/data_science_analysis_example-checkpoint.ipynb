{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic data analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### About load file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "INPUT_DIR = os.getcwd()\n",
    "file_name = \"data.csv\"\n",
    "\n",
    "load_csv = INPUT_DIR + file_name\n",
    "df = pd.read_csv(load_csv, index_col=0, parse_dates=True, dayfirst=True).sort_index(ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### General analysis / Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print head of data, so show example\n",
    "df.head()\n",
    "#print feature data type\n",
    "df.dtypes\n",
    "#print data sample number\n",
    "print(\"We have {:,} market samples in the training dataset.\".format(df.shape[0]))\n",
    "#print price value\n",
    "print(\"Average Standard deviation of price change within a day in {df['price'].mean():.4f}.\")\n",
    "#check nan value \n",
    "df.isna().sum()\n",
    "#check unique number, e.g. 'time' is one of feature\n",
    "df['time'].nunique()\n",
    "#check feature name equal to unknown\n",
    "df[df['time'] == 'unknown'].size\n",
    "#get detail information about feature\n",
    "df['time'].describe()\n",
    "#filter data through specific value condition\n",
    "outliers = df[(df['time']>1)|(df['time']<-1)]\n",
    "#groupby. Let 'time' feature be the key, other feature in the table become value\n",
    "df.groupby('time') \n",
    "#limit time period\n",
    "new_df = df.loc[df['time']>='2010-01-01 22:00:00+0000']\n",
    "#value_counts, pick top 10.\n",
    "df['feature'].value_counts().head(10)\n",
    "#plot a horizontal bar plot 水平柱状图\n",
    "ax = df.plot.barh(x='lab', y='val')\n",
    "df['time'].plot('barh')\n",
    "#plot bar 柱状图\n",
    "df['time'].plot('bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature analysis (category part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#onehot encode with scikit-learn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "data = ['warm', 'cold', 'hot']\n",
    "value = array(data)\n",
    "#integer encode\n",
    "label_encoder = LabelEncoder()\n",
    "#from 0,1,2,3,...\n",
    "integer_encoded = label_encoder.fit_transform(values)\n",
    "\n",
    "#binary encode\n",
    "onehot_encoder = OneHotEncoder(sparse= False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "\n",
    "#invert oneHot to label\n",
    "#here, [0,:] means the first example\n",
    "inverted = label_encoder.inverse_transform([argmax(onehot_encoded[0,:])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature analysis (text part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature analysis (Time series part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#log return\n",
    "def log_returns(x, lag=1):\n",
    "    \"\"\"Calculate log returns between adjacent close prices\"\"\"\n",
    "    return np.log(x) - np.log(x.shift(lag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#date process\n",
    "def to_date(date_column):\n",
    "    return data_column.apply(lambda x: pd.to_datetime(x).strftime('%d-%m-%Y'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#time frequency\n",
    "#resample and interpolate\n",
    "#two type resample method: Upsampling, Downsampling.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ETL extract, clean, transform, load\n",
    "#extract, combine available feature into one table\n",
    "def extract():\n",
    "    #df = pd.DataFrame()\n",
    "    lme_stocks = data['lme_stocks']\n",
    "    frames = [lme_stocks, lme_prices, ted_, ted_spread_, bdi_, vix_, gsci_, shfe_price]\n",
    "    df = pd.concat(frames, axis = 1)\n",
    "    df.columns = ['lme_stocks', 'lme_prices', 'ted', 'ted_spread', 'bdi', 'vix', 'gsci', 'shfe_price']\n",
    "    \n",
    "def clean():\n",
    "    logging.warning('Starting to clean data')\n",
    "    #remove the label is null\n",
    "    df.dropna(axis=0, how='any', subset=['shfe_price'], inplace=True)\n",
    "    check = df.isnull().values.any()\n",
    "    \n",
    "    # method 1: fill nan with average value\n",
    "    #if check:\n",
    "    #    mean = df.mean #mean of each column of the dataframe\n",
    "    #    replace_values = {'Open': mean[0], 'High':mean[1], 'Close':mean[2]}\n",
    "    #    df = df.fillna(value= replace_values)\n",
    "    #    \n",
    "    #Converting stock values to float and Volumns to int\n",
    "    #df[['Open', 'High', 'Close']] = df[['Open', 'High', 'Close']].astyoe(float)\n",
    "    #df['Volumn'] = df['Volumn'].astype(int)\n",
    "    #\n",
    "    #\n",
    "    # method 2: fill nan with last value\n",
    "    if check:\n",
    "        #fill nan by forward value \n",
    "        df = df.replace([np.inf, -np.inf], np.nan).fillna(method='ffill')\n",
    "    \n",
    "def transform():\n",
    "    logging.warning('Performing data transformation')\n",
    "    #making new column\n",
    "    #log return or percentage change\n",
    "    df['log_return_price'] = log_return(df['log_return_price'])\n",
    "    \n",
    "    # Find first and last dates for which all features available\n",
    "    start_date = df.loc[df.notnull().all(axis=1)].index.min()\n",
    "    end_date = df.loc[df.notnull().all(axis=1)].index.max()\n",
    "    df = df.loc[start_date:end_date, :]\n",
    "    \n",
    "    # Maybe no need\n",
    "    #normalised to zero mean and standard variance\n",
    "    #for item in df_Xy.columns:\n",
    "    #    df_Xy[item] = preprocessing.scale(df_Xy[item])     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check mean, std, and plot to show result.\n",
    "#function to plot multi-figures\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(10,10))\n",
    "axes[0,0].plot(cu_log_return.index,cu_log_return[['shfe_price']])\n",
    "axes[0,0].set_title('Cu_Shfe_price')\n",
    "axes[0,1].plot(cu_log_return.index,cu_log_return[['lme_stocks']])\n",
    "axes[0,1].set_title('Cu_lme_stocks')\n",
    "axes[1,0].plot(cu_log_return.index,cu_log_return[['lme_prices']])\n",
    "axes[1,0].set_title('Cu_lme_prices')\n",
    "axes[1,1].plot(cu_log_return.index,cu_log_return[['ted']])\n",
    "axes[1,1].set_title('ted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot autocorrelation and partial autocorrelation to show stationary.\n",
    "import statsmodels.api as sm  \n",
    "def plot_acf_pacf(input_data):   \n",
    "    fig = plt.figure(figsize=(12,8))\n",
    "    ax1 = fig.add_subplot(211)\n",
    "    fig = sm.graphics.tsa.plot_acf(input_data, lags=40, alpha=.05, ax=ax1)\n",
    "    ax2 = fig.add_subplot(212)\n",
    "    fig = sm.graphics.tsa.plot_pacf(input_data, lags=40, alpha=.05, ax=ax2)\n",
    "    print(\"ACF and PACF of input_data\")\n",
    "    plt.show()\n",
    "    \n",
    "#or use Dickey-Fuller test to check stationary.\n",
    "from pandas import tseries\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "def adf_test(input_data, feature_name):\n",
    "    # perform Augmented Dickey Fuller test\n",
    "    print('Results of Augmented Dickey-Fuller test:', feature_name)\n",
    "    test_data = input_data[[feature_name]]\n",
    "    y = test_data.values[:,0]\n",
    "    dftest = adfuller(y, autolag='AIC')\n",
    "    dfoutput = pd.Series(dftest[0:4], index=['test statistic', 'p-value', '# of lags', '# of observations'])\n",
    "    for key, value in dftest[4].items():\n",
    "        dfoutput['Critical Value ({})'.format(key)] = value\n",
    "    print(dftest)\n",
    "    #print(dfoutput)\n",
    "#     if dftest[1]<0.05 and dftest[4].get('5%')<0.05:\n",
    "#         print(feature_name, 'is stationary.')\n",
    "#         print('test statistic', dftest[0])\n",
    "#         print('p-value = ', dftest[1])\n",
    "#         print(type(dftest[1]))\n",
    "#         print('Critical Value (5%) = ', dftest[4].get('5%'))\n",
    "#     else: \n",
    "#         print(feature, 'is not stationary.')\n",
    "adf_test(al_log_return, 'shfe_price')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature cross correlation\n",
    "#method 1: numpy and panda\n",
    "corr = df.corr()\n",
    "corr.style.background_gradient()\n",
    "#also\n",
    "corr.style.background_gradient.set_precision(2)\n",
    "\n",
    "#method 2: \n",
    "import matplotlib.pyplot as plt\n",
    "#matplotlib.style.use('ggplot')\n",
    "plt.imshow(X.corr(), camp= plt.cm.Reds, interpolation='nearest')\n",
    "plt.colorbar()\n",
    "tick_marks = [i for i in range(len(X.columns))]\n",
    "plt.xticks(tick_marks, X.columns, rotation='vertical')\n",
    "plt.yticks(tick_marks, X.columns)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Unbalanced data set solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set weights?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = model_selection.train_test_split(X, Y, test_size=0.1, random_state=99)\n",
    "\n",
    "xgb = XGBClassifier(n_jobs=4, n_estimators=300, max_depth=6, eta=0.15)\n",
    "xgb.fit(X_train, Y_train)\n",
    "print(\"Accuracy Score: \", accuracy_score(xgb.predict(X_test), Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gaussian Process Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_process(X, Y, x_pred):\n",
    "       \n",
    "    X = X.values\n",
    "    Y = Y.values\n",
    "    x_pred = x_pred.values\n",
    "    kernels = 1.0 * DotProduct(sigma_0=1.0, sigma_0_bounds=(1e-05, 100000.0)) \\\n",
    "#     +1.0 * RationalQuadratic(length_scale=1.0) \\\n",
    "#     + 1.0 * WhiteKernel(noise_level=1e-1) \\\n",
    "#     + 1.0 * Matern(length_scale=1.0, nu=1.5) \\\n",
    "#     + 1.0 * RBF(length_scale=1.0, length_scale_bounds=(1e-05, 100000.0))\\\n",
    "#     + 1.0 * ExpSineSquared(length_scale=1.0, periodicity=1.0, length_scale_bounds=(1e-05, 100000.0), periodicity_bounds=(1e-05, 100000.0))\n",
    "#     + 1.0 * ConstantKernel(constant_value=1.0, constant_value_bounds=(1e-05, 100000.0))\\\n",
    "        \n",
    "    gp = GaussianProcessRegressor(kernel=kernels, n_restarts_optimizer=10, alpha = 0)\n",
    "    \n",
    "    # Fit to data using Maximum Likelihood Estimation of the parameters\n",
    "    # learn the hyperparameters and scale of each kernel\n",
    "    gp.fit(X, Y)\n",
    "\n",
    "    \n",
    "    forecast= gp.predict(x_pred, return_std=True, return_cov=False)\n",
    "    y_pred = forecast[0][0]\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Time series evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from numpy.linalg import norm\n",
    "from dtw import dtw\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "def evaluate(model_name, org_price, pred_price):\n",
    "    \n",
    "    org_price = org_price.reshape(len(org_price),1)\n",
    "    pred_price = pred_price.reshape(len(pred_price),1)\n",
    "    \n",
    "    mse = compute_mse_error(org_price, pred_price)\n",
    "    mae = compute_mean_absolute_error(org_price, pred_price)\n",
    "    rmse =compute_root_mean_square_error(org_price, pred_price)\n",
    "    dtw_dist, dtw_cost, dtw_acc, dtw_path = compute_dtw(org_price, pred_price)\n",
    "    corr_rank, p_value = compute_spearmanr(org_price, pred_price)\n",
    "    \n",
    "    df = pd.DataFrame(index=[model_name],columns=['MSE', 'MAE', 'RMSE' ,'DTW', 'Spearmanr', 'p-value'])\n",
    "    df.set_value(model_name, 'MSE', mse)\n",
    "    df.set_value(model_name, 'MAE', mae)\n",
    "    df.set_value(model_name, 'RMSE', rmse)\n",
    "    df.set_value(model_name, 'DTW', dtw_cost[-1][-1])\n",
    "    df.set_value(model_name, 'Spearmanr', corr_rank)\n",
    "    df.set_value(model_name, 'p_value', p_value)\n",
    "    return df\n",
    "\n",
    "def compute_daily_error(org_price, pred_price):\n",
    "    return org_price - pred_price\n",
    "\n",
    "#MSE\n",
    "def compute_mse_error(org_price, pred_price):\n",
    "    return mean_squared_error(org_price, pred_price)\n",
    "\n",
    "#MAE\n",
    "def compute_mean_absolute_error(org_price, pred_price):  \n",
    "    return mean_absolute_error(org_price, pred_price)\n",
    "\n",
    "#RMSE\n",
    "def compute_root_mean_square_error(org_price, pred_price):\n",
    "    return np.sqrt(((org_price - pred_price) ** 2).mean())\n",
    "\n",
    "#smaller dtw Accumulated Distortion(cost[-1][-1]) means more similar time series\n",
    "def compute_dtw(x, y):   #Dynamic time warping\n",
    "    dist, cost, acc, path = dtw(x, y, dist=lambda x, y: norm(x - y, ord=1))\n",
    "    return dist, cost, acc, path\n",
    "\n",
    "def compute_spearmanr(org_price, pred_price):\n",
    "    correlation_rank = spearmanr(org_price, pred_price)\n",
    "    return correlation_rank[0], correlation_rank[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
